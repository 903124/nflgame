#!/usr/bin/env python2

import argparse
import os
import os.path
import sys
import urllib2

from bs4 import BeautifulSoup

import nflgame

parser = argparse.ArgumentParser(
    description='Player data web scraper from NFL.com',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
aa = parser.add_argument
aa('mode', type=str, metavar='MODE',
    help='One of "download" or "parse". Modes are exclusive to prevent '
         'the download process from being interrupted. In "parse" mode, '
         'player JSON data will be echoed to stdout.')
aa('dir', type=str, metavar='DIRECTORY',
    help='The directory where the HTML files are/will be stored.')
conf = parser.parse_args()

assert conf.mode in ('download', 'parse'), \
    'Mode must be "download" or "parse".'

if conf.mode == 'download':
    if os.access(conf.dir, os.R_OK):
        print >> sys.stderr, 'Directory %s already exists.' % conf.dir
        sys.exit(1)
    os.mkdir(conf.dir)

    base_url = "http://www.nfl.com/players/profile?id=%s"
    players = {}
    # for p in nflgame.one(2011, 17, "NE", "BUF").players.passing(): 
    games = nflgame.games(2011) + nflgame.games(2012, preseason=True)
    for p in nflgame.combine(games):
        if p.playerid in players:
            continue
        players[p.playerid] = {
            'full_name': '',
            'team': '',
            'gsis_name': p.name,
            'gsid': p.playerid,
        }

    total = len(players)
    for i, (pid, info) in enumerate(players.iteritems(), 1):
        url = base_url % pid
        print url
        fpath = os.path.join(conf.dir, '%s.html' % pid)
        print >> open(fpath, 'w+'), urllib2.urlopen(url).read(),
        s = '\r%d/%d complete. (%0.2f%%)' % (i, total, float(i) / float(total))
        print s,

elif conf.mode == 'parse':
    soup = BeautifulSoup(open('gronk.html').read())
    for meta in soup.find_all('meta'):
        print meta.get('id'), meta.get('content')
else:
    assert False

