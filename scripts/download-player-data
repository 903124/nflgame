#!/usr/bin/env python2

import argparse
import json
import re
import sys
import urllib2

from bs4 import BeautifulSoup

import nflgame.schedule

roster_team_url = 'http://www.nfl.com/teams/roster?team=%s'

parser = argparse.ArgumentParser(
    description='Download player meta data from NFL.com.',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
aa = parser.add_argument
aa('--start-json', type=str,
   help='A JSON file to start with. Only players *not* in the JSON file will '
        'be fetched from NFL.com. This is useful when an initial download '
        'fails to retrieve meta data for every player.')
args = parser.parse_args()

def perr(s):
    print >> sys.stderr, s

def perri(s):
    print >> sys.stderr, s,

def get_html(url):
    try:
        return urllib2.urlopen(url).read()
    except urllib2.HTTPError:
        return None

def tryint(v):
    try:
        return int(v)
    except:
        return 0

def profile_id(url):
    m = re.search('/(\d+)/profile$', url)
    return int(m.group(1))

if __name__ == '__main__':
    output = {}
    downloaded = set()
    if args.start_json is not None:
        output = json.loads(open(args.start_json).read())
        for player in output.itervalues():
            downloaded.add(profile_id(player['profile_url']))

    teams = set()
    for (_, _, _, home, away), _ in nflgame.schedule.games:
        teams.add(home)
        teams.add(away)

    players = []
    total = len(teams)

    perr('Downloading team roster data...')
    for i, team in enumerate(teams, 1):
        s = '\r%d/%d complete. (%0.2f%%)' \
            % (i, total, 100 * float(i) / float(total))
        perri(s)

        html = get_html(roster_team_url % team)
        if html is None:
            perr('Could not get HTML for team %s.' % team)
            continue
        soup = BeautifulSoup(html)
        for a in soup.find_all('a', href=re.compile('^/player/.*')):
            tds = list(a.find_parent('tr').find_all('td'))
            data = map(lambda td: td.get_text().strip(), tds)
            assert len(data) == 9

            # If we've already downloaded this player, no need to do it again.
            if profile_id(tds[1].a['href']) in downloaded:
                continue

            players.append({
                'gsisid': None,
                'name': None,
                'team': team,
                'profile_url': 'http://www.nfl.com%s' % tds[1].a['href'],
                'number': data[0],
                'position': data[2],
                'status': data[3],
                'height': data[4],
                'weight': data[5],
                'birthdate': data[6],
                'years_pro': tryint(data[7]),
                'college': data[8],
            })

    total = len(players)
    perr('\nDownloading individual player data...')
    for i, player in enumerate(players, 1):
        s = '\r%d/%d complete. (%0.2f%%)' \
            % (i, total, 100 * float(i) / float(total))
        perri(s)

        html = get_html(player['profile_url'])
        if html is None:
            perr('Could not get HTML for URL %s' % player['profile_url'])
            continue

        soup = BeautifulSoup(html)
        try:
            name = soup.find('meta', id='playerName')['content'].strip()
        except TypeError:
            name = None
        try:
            gsisid = re.search('GSIS\s+ID:\s+([0-9-]+)', str(soup)).group(1)
            gsisid = gsisid.strip()
        except IndexError:
            gsisid = None

        player['gsisid'] = gsisid
        player['name'] = name

    # We only want to output players that have names/ids.
    for p in players:
        if p['gsisid'] is None or p['name'] is None:
            continue
        output[p['gsisid']] = p
    print json.dumps(output, indent=4, sort_keys=True)
    
    perr('\nDone.')
