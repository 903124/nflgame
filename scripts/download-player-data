#!/usr/bin/env python2

import argparse
import json
import os
import os.path
import sys
import urllib2

from bs4 import BeautifulSoup

import nflgame

parser = argparse.ArgumentParser(
    description='Player data web scraper from NFL.com',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
aa = parser.add_argument
aa('mode', type=str, metavar='MODE',
    help='One of "download" or "parse". Modes are exclusive to prevent '
         'the download process from being interrupted. In "parse" mode, '
         'player JSON data will be echoed to stdout.')
aa('dir', type=str, metavar='DIRECTORY',
    help='The directory where the HTML files are/will be stored.')
conf = parser.parse_args()

assert conf.mode in ('download', 'parse'), \
    'Mode must be "download" or "parse".'

players = {}
games = nflgame.games(2011) + nflgame.games(2012, preseason=True)
for p in nflgame.combine(games):
    if p.playerid in players:
        continue
    players[p.playerid] = {
        'full_name': '',
        'team': '',
        'gsis_name': p.name,
        'gsid': p.playerid,
    }

if conf.mode == 'download':
    if os.access(conf.dir, os.R_OK):
        print >> sys.stderr, 'Directory %s already exists.' % conf.dir
        sys.exit(1)
    os.mkdir(conf.dir)

    base_url = "http://www.nfl.com/players/profile?id=%s"

    total = len(players)
    for i, (pid, info) in enumerate(players.iteritems(), 1):
        url = base_url % pid
        fpath = os.path.join(conf.dir, '%s.html' % pid)
        try:
            html = urllib2.urlopen(url).read()
            print >> open(fpath, 'w+'), html,
        except urllib2.HTTPError:
            pass
        s = '\r%d/%d complete. (%0.2f%%)' % (i, total, float(i) / float(total))
        print s,

elif conf.mode == 'parse':
    files = os.listdir(conf.dir)
    total = len(files)
    for i, f in enumerate(os.listdir(conf.dir), 1):
        s = '\r%d/%d complete. (%0.2f%%)' % (i, total, float(i) / float(total))
        print s,

        if not f.endswith('.html'):
            continue

        playerid, _ = os.path.splitext(os.path.basename(f))
        player = players[playerid]
        soup = BeautifulSoup(open(os.path.join(conf.dir, f)).read())
        for meta in soup.find_all('meta'):
            kind, value = meta.get('id'), meta.get('content')
            if kind == 'playerName':
                player['full_name'] = value.strip()
            if kind == 'playerTeam':
                player['team'] = value.strip()
    print json.dumps(players, indent=4),
else:
    assert False

# select * from fantasysports.teams.roster where team_key='273.l.104464.t.11' 

