#!/usr/bin/env python2

# Here's an outline of how this program works.
# Firstly, we load a dictionary mapping GSIS identifier to a dictionary of
# player meta data. This comes from either the flag `json-update-file` or
# nflgame's "players.json" file. We then build a reverse map from profile
# identifier (included in player meta data) to GSIS identifier.
#
# We then look at all players who have participated in the last week of
# play. Any player in this set that is not in the aforementioned mapping
# has his GSIS identifier and name (e.g., `T.Brady`) added to a list of
# players to update.
#
# (N.B. When the initial mappings are empty, then every player who recorded
# a statistic in 2009 is added to this list.)
#
# For each player in the list to update, we need to obtain the profile
# identifier. This is done by sending a single HEAD request to the
# `gsis_profile` URL. The URL is a redirect to their canonical profile page,
# with which we extract the profile id. We add this mapping to both of the
# mappings discussed previously. (But note that the meta data in the GSIS
# identifier mapping is incomplete.)
#
# We now fetch the roster lists for each of the 32 teams from NFL.com.
# The roster list contains all relevant meta data *except* the GSIS identifier.
# However, since we have a profile identifier for each player (which is
# included in the roster list), we can connect player meta data with a
# particular GSIS identifier. If we happen to see a player on the roster that
# isn't in the mapping from profile identifier to GSIS identifier, then we need
# to do a full GET request on that player's profile to retrieve the GSIS
# identifier. (This occurs when a player has been added to a roster but hasn't
# recorded any statistics yet.)
#
# We update the initial dictionary of player meta data for each player,
# including adding new entries for new players. We then save the updated
# mapping from GSIS identifier to player meta data to disk as JSON.
# (The JSON dump is sorted by key so that diffs are meaningful.)
#
# This approach requires a few thousand HEAD requests to NFL.com on the first
# run. But after that, most runs will only require 32 requests for the roster
# list (small potatoes) and perhaps a few HEAD requests if there happens to be
# a new player found.

from __future__ import absolute_import, division, print_function
import argparse
import json
import re
import sys

import eventlet

httplib2 = eventlet.import_patched('httplib2')

from bs4 import BeautifulSoup

import nflgame
import nflgame.live
import nflgame.player

parser = argparse.ArgumentParser(
    description='Efficiently download player meta data from NFL.com. Note '
                'that each invocation of this program guarantees at least '
                '32 HTTP requests to NFL.com',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
aa = parser.add_argument
aa('--json-update-file', type=str, default=None,
   help='When set, the file provided will be updated in place with new meta '
        'data from NFL.com. If this option is not set, then the '
        '"players.json" file that comes with nflgame will be updated instead.')
aa('--simultaneous-reqs', type=int, default=3,
   help='The number of simultaneous HTTP requests sent to NFL.com at a time. '
        'Set this lower if you are worried about hitting their servers.')
aa('--full-scan', action='store_true',
   help='Forces a full scan of nflgame player data since 2009. Typically, '
        'this is only done when starting with a fresh JSON player database. '
        'But it can be useful to re-scan all of the players if past errors '
        'went ignored and data is missing. The advantage of using this option '
        'over starting fresh is that an existing (gsis_id <-> profile_id) '
        'mapping can be used for the majority of players, instead of querying '
        'NFL.com for the mapping all over again.')
args = parser.parse_args()

if args.json_update_file is None:
    args.json_update_file = nflgame.player._player_json_file
urls = {
    'roster': 'http://www.nfl.com/teams/roster?team=%s',
    'gsis_profile': 'http://www.nfl.com/players/profile?id=%s',
}
teams = [team[0] for team in nflgame.teams]
green_pool = eventlet.GreenPool(args.simultaneous_reqs)


def new_http():
    http = httplib2.Http(timeout=5)
    http.follow_redirects = False
    return http


def initial_mappings():
    metas, reverse = {}, {}
    try:
        with open(args.json_update_file) as fp:
            metas = json.load(fp)
        for gsis_id, meta in metas.items():
            reverse[meta['profile_id']] = gsis_id
    except IOError as e:
        eprint('Could not open "%s": %s' % (args.json_update_file, e))
    return metas, reverse


def profile_id_from_url(url):
    m = re.search('/([0-9]+)/', url)
    return None if m is None else int(m.group(1))


def profile_id(gsis_id):
    resp, content = new_http().request(urls['gsis_profile'] % gsis_id, 'HEAD')
    if resp['status'] != '301':
        return None
    return profile_id_from_url(resp['location'])


def gsis_id(profile_url):
    resp, content = new_http().request(profile_url, 'GET')
    if resp['status'] != '200':
        return None
    m = re.search('GSIS\s+ID:\s+([0-9-]+)', content)
    return None if m is None else m.group(1)


def roster_soup(team):
    resp, content = new_http().request(urls['roster'] % team, 'GET')
    if resp['status'] != '200':
        return None
    return BeautifulSoup(content)


def try_int(s):
    try:
        return int(s)
    except ValueError:
        return 0


def meta_from_soup_row(team, soup_row):
    tds, data = [], []
    for td in soup_row.find_all('td'):
        tds.append(td)
        data.append(td.get_text().strip())
    profile_url = 'http://www.nfl.com%s' % tds[1].a['href']

    name = tds[1].a.get_text().strip()
    if ',' not in name:
        last_name, first_name = name, ''
    else:
        last_name, first_name = map(lambda s: s.strip(), name.split(','))
    return {
        'team': team,
        'profile_id': profile_id_from_url(profile_url),
        'profile_url': profile_url,
        'number': try_int(data[0]),
        'first_name': first_name,
        'last_name': last_name,
        'full_name': '%s %s' % (first_name, last_name),
        'position': data[2],
        'status': data[3],
        'height': data[4],
        'weight': data[5],
        'birthdate': data[6],
        'years_pro': try_int(data[7]),
        'college': data[8],
    }


def players_from_games(existing, games):
    for g in games:
        for d in g.drives:
            for p in d.plays:
                for player in p.players:
                    if player.playerid not in existing:
                        yield player.playerid, player.name


def eprint(*args, **kwargs):
    kwargs['file'] = sys.stderr
    print(*args, **kwargs)


def progress(cur, total):
    ratio = 100 * (float(cur) / float(total))
    eprint('\r%d/%d complete. (%0.2f%%)' % (cur, total, ratio), end='')


def progress_done():
    eprint('\nDone!')


# Fetch the initial mapping of players.
metas, reverse = initial_mappings()
if len(metas) == 0:
    eprint("nflgame doesn't know about any players.")
    eprint("Updating player data will require several thousand HTTP HEAD "
           "requests to NFL.com.")
    eprint("It is strongly recommended to find the 'players.json' file that "
           "comes with nflgame.")
    eprint("Are you sure you want to continue? [y/n] ", end='')
    answer = raw_input()
    if answer[0].lower() != 'y':
        eprint("Quitting...")
        sys.exit(1)

# Accumulate errors as we go. Dump them at the end.
errors = []

# Now fetch a set of players that aren't in our mapping already.
# Restrict the search to the current week if we have a non-empty mapping.
if len(metas) == 0 or args.full_scan:
    eprint('Loading players in games since 2009, this may take a while...')
    players = {}
    for game_id in nflgame.schedule.games_byid:
        if nflgame.schedule.games_byid[game_id]['year'] != 2012:
            continue
        for pid, nm in players_from_games(metas, [nflgame.game.Game(game_id)]):
            players[pid] = nm
    eprint('Done.')
else:
    nflgame.live._update_week_number()
    phase = nflgame.live._cur_season_phase
    year, week = nflgame.live.current_year_and_week()
    eprint('Loading games for %s %d week %d' %  (phase, year, week))
    games = nflgame.games(year, week, kind=phase)
    players = dict(players_from_games(metas, games))

# Find the profile ID for each new player.
if len(players) > 0:
    eprint('Finding (profile id -> gsis id) mapping for players...')
    def fetch(t):  # t[0] is the gsis_id and t[1] is the gsis name
        return t[0], t[1], profile_id(t[0])
    for i, t in enumerate(green_pool.imap(fetch, players.items()), 1):
        gid, name, pid = t
        progress(i, len(players))
        if pid is None:
            errors.append('Could not get profile id for (%s, %s)'
                          % (gid, name))
            continue

        assert gid not in metas
        metas[gid] = { 'gsis_id': gid, 'gsis_name': name, 'profile_id': pid }
        reverse[pid] = gid
    progress_done()

# Get the soup for each team roster.
eprint('Downloading team rosters...')
roster = []
def fetch(team):
    return team, roster_soup(team)
for i, (team, soup) in enumerate(green_pool.imap(fetch, teams), 1):
    progress(i, len(teams))

    if soup is None:
        errors.append('Could not get roster for team %s' % team)
        continue
    for row in soup.find(id='result').find('tbody').find_all('tr'):
        roster.append(meta_from_soup_row(team, row))
progress_done()

# Find the gsis identifiers for players that are in the roster but haven't
# recorded a statistic yet. (i.e., Not in nflgame play data.)
purls = [r['profile_url'] for r in roster if r['profile_id'] not in reverse]
if len(purls) > 0:
    eprint('Fetching GSIS identifiers for players not in nflgame...')
    def fetch(purl):
        return purl, gsis_id(purl)
    for i, (purl, gid) in enumerate(green_pool.imap(fetch, purls), 1):
        progress(i, len(purls))

        if gid is None:
            errors.append('Could not get GSIS id at %s' % purl)
            continue
        reverse[profile_id_from_url(purl)] = gid
    progress_done()

# Now merge the data from `rosters` into `metas` by using `reverse` to
# establish the correspondence.
for data in roster:
    gsisid = reverse.get(data['profile_id'], None)
    if gsisid is None:
        errors.append('Could not find gsis_id for %s at %s'
                      % (data['name'], data['profile_url']))
        continue
    merged = dict(metas.get(gsisid, {}), **data)
    merged['gsis_id'] = gsisid
    metas[gsisid] = merged

assert len(metas) > 0, "Have no players to add... ???"
with open(args.json_update_file, 'w+') as fp:
    json.dump(metas, fp, indent=4, sort_keys=True)

if len(errors) > 0:
    eprint('There were some errors during the download. Usually this is a\n'
           'result of an HTTP request timing out, which means the resulting\n'
           'players.json file is probably missing some data.\n'
           'An appropriate solution is to re-run the script until there are\n'
           'no more errors.')
    eprint('-' * 79)
    eprint(('\n' + ('-' * 79) + '\n').join(errors))

